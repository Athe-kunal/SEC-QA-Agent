No sentence-transformers model found with name /home/athekunal/.cache/torch/sentence_transformers/ProsusAI_finbert. Creating a new one with MEAN pooling.
Some weights of the model checkpoint at /home/athekunal/.cache/torch/sentence_transformers/ProsusAI_finbert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/home/athekunal/SEC-Project/wandb_MLOPs/main.py", line 11, in <module>
    llm1_output_dict, query_metadata = get_response_llm1(USER_REQUEST,"10-K")
  File "/home/athekunal/SEC-Project/wandb_MLOPs/LLM1.py", line 85, in get_response_llm1
    output_1 = llm1.predict(llm1_prompt)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 467, in predict
    return self(text, stop=_stop, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 427, in __call__
    self.generate(
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 279, in generate
    output = self._generate_helper(
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 223, in _generate_helper
    raise e
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 210, in _generate_helper
    self._generate(
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/openai.py", line 320, in _generate
    response = completion_with_retry(self, prompt=_prompts, **params)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/openai.py", line 90, in completion_with_retry
    return _completion_with_retry(**kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/openai.py", line 88, in _completion_with_retry
    return llm.client.create(**kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/wandb/sdk/integration_utils/auto_logging.py", line 92, in method
    result = original_method(*args, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_resources/completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?
Traceback (most recent call last):
  File "/home/athekunal/SEC-Project/wandb_MLOPs/main.py", line 11, in <module>
    llm1_output_dict, query_metadata = get_response_llm1(USER_REQUEST,"10-K")
  File "/home/athekunal/SEC-Project/wandb_MLOPs/LLM1.py", line 85, in get_response_llm1
    output_1 = llm1.predict(llm1_prompt)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 467, in predict
    return self(text, stop=_stop, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 427, in __call__
    self.generate(
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 279, in generate
    output = self._generate_helper(
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 223, in _generate_helper
    raise e
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/base.py", line 210, in _generate_helper
    self._generate(
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/openai.py", line 320, in _generate
    response = completion_with_retry(self, prompt=_prompts, **params)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/openai.py", line 90, in completion_with_retry
    return _completion_with_retry(**kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 289, in wrapped_f
    return self(f, *args, **kw)
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 379, in __call__
    do = self.iter(retry_state=retry_state)
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 314, in iter
    return fut.result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/athekunal/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 382, in __call__
    result = fn(*args, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/langchain/llms/openai.py", line 88, in _completion_with_retry
    return llm.client.create(**kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/wandb/sdk/integration_utils/auto_logging.py", line 92, in method
    result = original_method(*args, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_resources/completion.py", line 25, in create
    return super().create(*args, **kwargs)
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/home/athekunal/.local/lib/python3.10/site-packages/openai/api_requestor.py", line 763, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?